{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aca85a4",
   "metadata": {},
   "source": [
    "Lab Assignment 3\n",
    "\n",
    "Muhammad Ammar Wafiy - IS01082517\n",
    "Nur Aisya Safiyyah - IS01082522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c375251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014aa3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ammar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ammar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ammar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d07256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 11314 records and 5 columns.\n",
      "Dataset after removing null values: 11096 records\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('news_dataset.csv')\n",
    "print(f'The dataset contains {data.shape[0]} records and {data.shape[1]} columns.')\n",
    "data = data[['text']].dropna()\n",
    "print(f'Dataset after removing null values: {data.shape[0]} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6ed7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and 2 < len(word) <= 20 and word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ee0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of processed text:\n",
      "0    wondering anyone could enlighten car saw day d...\n",
      "1    recently posted article asking kind rate singl...\n",
      "2    depends priority lot people put higher priorit...\n",
      "3    excellent automatic found subaru legacy switch...\n",
      "4    ford automobile need information whether ford ...\n",
      "5    watch attributionsi didnt say isnt appropriate...\n",
      "6    avoid problem entirely installing oil drain va...\n",
      "7    acura integra speed mile positively worst car ...\n",
      "8    assuming non turbo gruffness characteristic la...\n",
      "9    addition restricted mileage many classic insur...\n",
      "Name: processed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "print(\"\\nSample of processed text:\")\n",
    "print(data['processed_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d51f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique tokens : 73869\n"
     ]
    }
   ],
   "source": [
    "bow_dictionary = corpora.Dictionary([text.split() for text in data['processed_text']])\n",
    "bow_corpus = [bow_dictionary.doc2bow(text.split()) for text in data['processed_text']]\n",
    "print(f\"\\nNumber of unique tokens : {len(bow_dictionary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a378d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.5)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c294dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = [[(i, float(tfidf_matrix[row, i])) for i in tfidf_matrix[row].nonzero()[1]] for row in range(tfidf_matrix.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5429e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_corpus = []\n",
    "for bow_doc, tfidf_doc in zip(bow_corpus, tfidf_corpus):\n",
    "    combined = {}\n",
    "    for word_id, bow_weight in bow_doc:\n",
    "        combined[word_id] = bow_weight * 0.5\n",
    "    for word_id, tfidf_weight in tfidf_doc:\n",
    "        combined[word_id] = combined.get(word_id, 0) + tfidf_weight * 0.5\n",
    "    combined_corpus.append(list(combined.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a50b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_combined = LdaModel(corpus=combined_corpus, id2word=bow_dictionary, num_topics=4, passes=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d79fa75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Topics :\n",
      "Topic 1: would, one, people, dont, think, know, like, time, get, say\n",
      "Topic 2: file, window, use, program, one, get, system, version, problem, thanks\n",
      "Topic 3: team, game, league, season, player, play, hockey, win, period, san\n",
      "Topic 4: key, encryption, chip, system, use, information, clipper, privacy, security, data\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracted Topics :\")\n",
    "for idx, topic in lda_model_combined.show_topics(num_topics=4, formatted=False):\n",
    "    topic_words = [word for word, _ in topic]\n",
    "    print(f\"Topic {idx+1}: {', '.join(topic_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f56d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_cv = CoherenceModel(model=lda_model_combined, texts=[text.split() for text in data['processed_text']], dictionary=bow_dictionary, coherence='c_v').get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20e6b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Coherence Score: 0.6350\n"
     ]
    }
   ],
   "source": [
    "print(f\"CV Coherence Score: {coherence_cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226c0e3",
   "metadata": {},
   "source": [
    "The CV coherence score of 0.6350 indicates that the topics generated by the LDA model are moderately coherent and reasonably interpretable. This score suggests that the words within each topic have a decent level of semantic similarity, meaning the topics are meaningful and relevant but not highly optimized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
